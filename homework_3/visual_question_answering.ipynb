{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNDL_Homework3\n",
    "\n",
    "\n",
    "### Introduction\n",
    "\n",
    "After having analysed carefully the code presented in class, we searched online for a few solutions and we took advantage of the code presented in this GitHub [repository](https://github.com/moduIo/Relation-Networks): an implementation of Relation Networks for Visual Question Answering using the CLEVR dataset; this repository is in turn inspired by the following [research](https://arxiv.org/pdf/1706.01427.pdf).\n",
    "\n",
    "We divided our dataset in train and validation (80-20 split) using the function train_test_split from sklearn library. \n",
    "\n",
    "The fixed random seed present in the code makes our results reproducible.\n",
    "\n",
    "Images are preprocessed: they are resized to 128x128 pixels.\n",
    "\n",
    "\n",
    "### Image Processing\n",
    "\n",
    "Since we cannot use the Default Keras' data augmentation function, we resize manually all the images by calling *process_image* function. \n",
    "We tried to implement a sort of data augmentation function by simply rotating the tensors (the 128x128 preprocessed images) but we encountered a library problem (we were using a deprecated function).\n",
    "We searched online to solve this problem but we didn't find anything. \n",
    "We are quite sure that by simply adding this the accuracy would have increased. \n",
    "\n",
    "\n",
    "### Model fitting \n",
    "\n",
    "Since we are using a custom data generator, we have to pass it directly to the model.fit function. \n",
    "The data generator function (*load_data_generator*), instead of reading the entire dataset, reads *n* times a batch of randomly sampled images (with relative question and answer). \n",
    "The accuracy was not that bad but it probably overfits quickly since the same images may be randomly sampled more than once.\n",
    "Another problem, in our opinion, is that some answers in the training dataset appear only few times and some of them may end-up (after the split) in the validation part. \n",
    "This causes the problem of never having trained the model with some cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import json\n",
    "import os.path\n",
    "import random as ra\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.optimizers import Adam\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Dropout, BatchNormalization, Reshape, Lambda, Embedding, LSTM, Conv2D, MaxPooling2D, TimeDistributed, RepeatVector, Concatenate\n",
    "from keras.models import Model\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "from scipy import ndimage, misc\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import tqdm\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Set the seed for random operations. \n",
    "# This let our experiments to be reproducible. \n",
    "SEED = 1234\n",
    "tf.random.set_seed(SEED)  \n",
    "\n",
    "# Get current working directory\n",
    "cwd = os.getcwd()\n",
    "\n",
    "# Set GPU memory growth\n",
    "# Allows to only as much GPU memory as needed\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "    for gpu in gpus:\n",
    "      tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "    print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "  except RuntimeError as e:\n",
    "    # Memory growth must be set before GPUs have been initialized\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = 7000\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "learning_rate = .00025\n",
    "vocab_size = 1024\n",
    "sequence_length = 64\n",
    "img_rows, img_cols = 320, 480\n",
    "image_input_shape = (img_rows, img_cols, 3)\n",
    "num_labels = 13\n",
    "current = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_image(x):\n",
    "    target_height, target_width = 128, 128\n",
    "    x = tf.image.resize(x, (target_height, target_width), method=tf.image.ResizeMethod.AREA)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relation_vectors(x):\n",
    "    objects = []\n",
    "    relations = []\n",
    "    shape = K.int_shape(x)\n",
    "    k = 25     # Hyperparameter which controls how many objects are considered\n",
    "    keys = []\n",
    "\n",
    "    # Get k unique random objects\n",
    "    while k > 0:\n",
    "        i = ra.randint(0, shape[1] - 1)\n",
    "        j = ra.randint(0, shape[2] - 1)\n",
    "\n",
    "        if not (i, j) in keys:\n",
    "            keys.append((i, j))\n",
    "            objects.append(x[:, i, j, :])\n",
    "            k -= 1\n",
    "\n",
    "    # Concatenate each pair of objects to form a relation vector\n",
    "    for i in range(len(objects)):\n",
    "        for j in range(i, len(objects)):\n",
    "            relations.append(K.concatenate([objects[i], objects[j]], axis=1))\n",
    "\n",
    "    # Restack objects into Keras tensor [batch, relation_ID, relation_vectors]\n",
    "    return K.permute_dimensions(K.stack([r for r in relations], axis=0), [1, 0, 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks=[]\n",
    "early_stop = True\n",
    "\n",
    "if early_stop:\n",
    "    es_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "    callbacks.append(es_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_inputs = Input(shape=(sequence_length,), name='text_input')\n",
    "text_x = Embedding(vocab_size, 128)(text_inputs)\n",
    "text_x = LSTM(128)(text_x)\n",
    "\n",
    "image_inputs = Input(shape=image_input_shape, name='image_input')\n",
    "image_x = Lambda(process_image)(image_inputs)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "image_x = Conv2D(24, kernel_size=(3, 3), strides=2, activation='relu')(image_x)\n",
    "image_x = BatchNormalization()(image_x)\n",
    "shape = K.int_shape(image_x)\n",
    "\n",
    "RN_inputs = Input(shape=(1, (2 * shape[3]) + K.int_shape(text_x)[1]))\n",
    "RN_x = Dense(256, activation='relu')(RN_inputs)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dense(256, activation='relu')(RN_x)\n",
    "RN_x = Dropout(.5)(RN_x)\n",
    "RN_outputs = Dense(256, activation='relu')(RN_x)\n",
    "RN = Model(inputs=RN_inputs, outputs=RN_outputs)\n",
    "\n",
    "relations = Lambda(get_relation_vectors)(image_x)           # Get tensor [batch, relation_ID, relation_vectors]\n",
    "question = RepeatVector(K.int_shape(relations)[1])(text_x)  # Shape question vector to same size as relations\n",
    "relations = Concatenate(axis=2)([relations, question])      # Merge tensors [batch, relation_ID, relation_vectors, question_vector]\n",
    "g = TimeDistributed(RN)(relations)                          # TimeDistributed applies RN to relation vectors.\n",
    "g = Lambda(lambda x: K.sum(x, axis=1))(g)                   # Sum over relation_ID\n",
    "\n",
    "f = Dense(256, activation='relu')(g)\n",
    "f = Dropout(.5)(f)\n",
    "f = Dense(256, activation='relu')(f)\n",
    "f = Dropout(.5)(f)\n",
    "outputs = Dense(num_labels, activation='softmax')(f)\n",
    "model = Model(inputs=[text_inputs, image_inputs], outputs=outputs)\n",
    "model.compile(optimizer=Adam(lr=learning_rate),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_generator(data, n=64):\n",
    "    current = 0\n",
    "    while True:\n",
    "        path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "        questions_path = path + '/train_data.json'\n",
    "        images_path = path + '/train/'\n",
    "        tokenize = None\n",
    "        batch_data = []\n",
    "        x_text = []     # List of questions\n",
    "        x_image = []    # List of images\n",
    "        y = []          # List of answers\n",
    "        num_labels = 0  # Current number of labels, used to create index mapping\n",
    "        labels = {}     # Dictionary mapping of ints to labels\n",
    "        images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "        \n",
    "        current_data = data [current:current+n]\n",
    "        current += n\n",
    "        if current+n == len(data):\n",
    "            current = 0\n",
    "            \n",
    "        batch_data.append(current_data)\n",
    "\n",
    "        labels= {'0': 0, '1': 1, '10': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, 'no': 11, 'yes': 12}\n",
    "\n",
    "        for q in batch_data[0][0:n]:\n",
    "            if not q['image_filename'] in images:\n",
    "                images[q['image_filename']] = imageio.imread(images_path + q['image_filename'], pilmode=\"RGB\")\n",
    "\n",
    "            x_text.append(q['question'])\n",
    "            x_image.append(images[q['image_filename']])\n",
    "            y.append(labels[q['answer']])\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "        tokenizer.fit_on_texts(x_text)\n",
    "        sequences = tokenizer.texts_to_sequences(x_text)\n",
    "        x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "        # Convert x_image to np array\n",
    "        x_image = np.array(x_image)\n",
    "\n",
    "        # Convert labels to categorical labels\n",
    "        y = keras.utils.to_categorical(y, num_labels)\n",
    "        yield ([x_text, x_image], y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_test(n, vocab_size, sequence_length, tokenizer=None):\n",
    "    path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "    questions_path = path + '/test_data.json'\n",
    "    images_path = path + '/test/'\n",
    "\n",
    "    x_text = []     # List of questions\n",
    "    x_image = []    # List of images\n",
    "    num_labels = 0  # Current number of labels, used to create index mapping\n",
    "    labels = {}     # Dictionary mapping of ints to labels\n",
    "    images = {}     # Dictionary of images, to minimize number of imread ops\n",
    "\n",
    "    # Attempt to load saved JSON subset of the questions\n",
    "    print('Loading data...')\n",
    "        \n",
    "    with open(questions_path) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    data = data['questions'][0:n]\n",
    "    \n",
    "    for q in data[0:n]:\n",
    "        # Create an index for each image\n",
    "        if not q['image_filename'] in images:\n",
    "            images[q['image_filename']] = imageio.imread(images_path + q['image_filename'], pilmode=\"RGB\")\n",
    "\n",
    "        x_text.append(q['question'])\n",
    "        x_image.append(images[q['image_filename']])\n",
    "        \n",
    "    # Convert question corpus into sequential encoding for LSTM\n",
    "    print('Processing text data...')\n",
    "    if not tokenizer:\n",
    "        tokenizer = Tokenizer(num_words=vocab_size)\n",
    "\n",
    "    tokenizer.fit_on_texts(x_text)\n",
    "    sequences = tokenizer.texts_to_sequences(x_text)\n",
    "    x_text = sequence.pad_sequences(sequences, maxlen=sequence_length)\n",
    "\n",
    "    # Convert x_image to np array\n",
    "    x_image = np.array(x_image)\n",
    "\n",
    "    print('Text: ', x_text.shape)\n",
    "    print('Image: ', x_image.shape)\n",
    "\n",
    "    return ([x_text, x_image]), tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/kaggle/input/ann-and-dl-vqa/dataset_vqa/'\n",
    "questions_path = path + '/train_data.json'\n",
    "\n",
    "with open(questions_path) as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data = data['questions']\n",
    "data_train, data_valid= train_test_split(data, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(load_data_generator(data_train),\n",
    "          epochs=epochs,\n",
    "          steps_per_epoch=len(data_train)//batch_size,\n",
    "          validation_data = load_data_generator(data_valid),\n",
    "          callbacks=callbacks,\n",
    "          validation_steps=len(data_valid)//batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def create_csv(results, results_dir='./'):\n",
    "\n",
    "    csv_fname = 'results_'\n",
    "    csv_fname += datetime.now().strftime('%b%d_%H-%M-%S') + '.csv'\n",
    "\n",
    "    with open(os.path.join(results_dir, csv_fname), 'w') as f:\n",
    "\n",
    "        f.write('Id,Category\\n')\n",
    "\n",
    "        for key, value in results.items():\n",
    "            f.write(str(key) + ',' + str(value) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(test), tok = load_data_test(samples, vocab_size, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_softmax = model.predict(test)\n",
    "predicted_class = out_softmax.argmax(axis=-1)\n",
    "\n",
    "d = {}\n",
    "for i in range(0, 3000):\n",
    "    d[i] = predicted_class[i]\n",
    "\n",
    "create_csv(d)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
